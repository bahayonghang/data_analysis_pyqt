#!/usr/bin/env python3
"""
ä»»åŠ¡13.1: æ•°æ®å¤„ç†ç®—æ³•å‡†ç¡®æ€§æµ‹è¯•

æµ‹è¯•æ‰€æœ‰æ•°æ®å¤„ç†å’Œåˆ†æç®—æ³•çš„å‡†ç¡®æ€§ï¼š
1. æè¿°æ€§ç»Ÿè®¡ç®—æ³•æµ‹è¯•
2. ç›¸å…³æ€§åˆ†æç®—æ³•æµ‹è¯•
3. å¼‚å¸¸å€¼æ£€æµ‹ç®—æ³•æµ‹è¯•
4. æ—¶é—´åºåˆ—åˆ†æç®—æ³•æµ‹è¯•
5. æ•°æ®é¢„å¤„ç†ç®—æ³•æµ‹è¯•
"""

import sys
import unittest
import numpy as np
import pandas as pd
from pathlib import Path
from scipy import stats
from sklearn.preprocessing import StandardScaler
import warnings
warnings.filterwarnings('ignore')

# å¯¼å…¥æµ‹è¯•é…ç½®
from .test_config import TestConfig, TestDataGenerator, TestAssertions

# æ·»åŠ srcç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root / "src"))


class TestDescriptiveStatistics(unittest.TestCase):
    """æµ‹è¯•æè¿°æ€§ç»Ÿè®¡ç®—æ³•"""
    
    def setUp(self):
        """æµ‹è¯•è®¾ç½®"""
        TestConfig.setup()
    
    def tearDown(self):
        """æµ‹è¯•æ¸…ç†"""
        TestConfig.teardown()
    
    def test_descriptive_stats_accuracy(self):
        """æµ‹è¯•æè¿°æ€§ç»Ÿè®¡çš„å‡†ç¡®æ€§"""
        from src.core.analysis_engine import AnalysisEngine, AnalysisConfig
        
        # åˆ›å»ºå·²çŸ¥ç»Ÿè®¡ç‰¹æ€§çš„æ•°æ®
        np.random.seed(42)
        data = np.random.normal(100, 15, 1000)  # å‡å€¼100ï¼Œæ ‡å‡†å·®15
        df = pd.DataFrame({'value': data})
        
        # æ‰§è¡Œåˆ†æ
        config = AnalysisConfig(include_descriptive=True)
        engine = AnalysisEngine(config)
        result = engine.analyze(df)
        
        # éªŒè¯ç»Ÿè®¡ç»“æœçš„å‡†ç¡®æ€§
        stats_dict = result.descriptive_stats.get('value', {})
        
        # æ£€æŸ¥å‡å€¼ï¼ˆå…è®¸5%è¯¯å·®ï¼‰
        calculated_mean = stats_dict.get('mean', 0)
        expected_mean = 100
        mean_error = abs(calculated_mean - expected_mean) / expected_mean
        self.assertLess(mean_error, 0.05, f"å‡å€¼è¯¯å·®è¿‡å¤§: {mean_error:.3f}")
        
        # æ£€æŸ¥æ ‡å‡†å·®ï¼ˆå…è®¸10%è¯¯å·®ï¼‰
        calculated_std = stats_dict.get('std', 0)
        expected_std = 15
        std_error = abs(calculated_std - expected_std) / expected_std
        self.assertLess(std_error, 0.10, f"æ ‡å‡†å·®è¯¯å·®è¿‡å¤§: {std_error:.3f}")
        
        print(f"âœ… æè¿°æ€§ç»Ÿè®¡å‡†ç¡®æ€§æµ‹è¯•é€šè¿‡")
        print(f"   è®¡ç®—å‡å€¼: {calculated_mean:.2f} (æœŸæœ›: {expected_mean})")
        print(f"   è®¡ç®—æ ‡å‡†å·®: {calculated_std:.2f} (æœŸæœ›: {expected_std})")
    
    def test_quartiles_calculation(self):
        """æµ‹è¯•å››åˆ†ä½æ•°è®¡ç®—"""
        from src.core.analysis_engine import AnalysisEngine, AnalysisConfig
        
        # åˆ›å»ºå·²çŸ¥åˆ†ä½æ•°çš„æ•°æ®
        data = list(range(1, 101))  # 1åˆ°100
        df = pd.DataFrame({'value': data})
        
        config = AnalysisConfig(include_descriptive=True)
        engine = AnalysisEngine(config)
        result = engine.analyze(df)
        
        stats_dict = result.descriptive_stats.get('value', {})
        
        # éªŒè¯åˆ†ä½æ•°
        q25 = stats_dict.get('25%', 0)
        q50 = stats_dict.get('50%', 0)  # ä¸­ä½æ•°
        q75 = stats_dict.get('75%', 0)
        
        # å¯¹äº1-100çš„æ•°æ®ï¼ŒæœŸæœ›çš„åˆ†ä½æ•°
        expected_q25 = 25.75  # pandasé»˜è®¤æ’å€¼æ–¹æ³•
        expected_q50 = 50.5
        expected_q75 = 75.25
        
        self.assertAlmostEqual(q25, expected_q25, places=1, msg="25%åˆ†ä½æ•°ä¸å‡†ç¡®")
        self.assertAlmostEqual(q50, expected_q50, places=1, msg="ä¸­ä½æ•°ä¸å‡†ç¡®")
        self.assertAlmostEqual(q75, expected_q75, places=1, msg="75%åˆ†ä½æ•°ä¸å‡†ç¡®")
        
        print("âœ… å››åˆ†ä½æ•°è®¡ç®—å‡†ç¡®æ€§æµ‹è¯•é€šè¿‡")


class TestCorrelationAnalysis(unittest.TestCase):
    """æµ‹è¯•ç›¸å…³æ€§åˆ†æç®—æ³•"""
    
    def setUp(self):
        """æµ‹è¯•è®¾ç½®"""
        TestConfig.setup()
    
    def tearDown(self):
        """æµ‹è¯•æ¸…ç†"""
        TestConfig.teardown()
    
    def test_perfect_correlation(self):
        """æµ‹è¯•å®Œå…¨ç›¸å…³çš„æƒ…å†µ"""
        from src.core.analysis_engine import AnalysisEngine, AnalysisConfig
        
        # åˆ›å»ºå®Œå…¨æ­£ç›¸å…³çš„æ•°æ®
        x = np.array([1, 2, 3, 4, 5])
        y = 2 * x + 1  # å®Œå…¨çº¿æ€§ç›¸å…³
        df = pd.DataFrame({'x': x, 'y': y})
        
        config = AnalysisConfig(
            include_correlation=True,
            correlation_method='pearson'
        )
        engine = AnalysisEngine(config)
        result = engine.analyze(df)
        
        # æ£€æŸ¥ç›¸å…³ç³»æ•°
        corr_matrix = result.correlation_matrix
        if isinstance(corr_matrix, dict):
            # æå–xå’Œyä¹‹é—´çš„ç›¸å…³ç³»æ•°
            xy_correlation = None
            if 'x' in corr_matrix and 'y' in corr_matrix['x']:
                xy_correlation = corr_matrix['x']['y']
            elif 'y' in corr_matrix and 'x' in corr_matrix['y']:
                xy_correlation = corr_matrix['y']['x']
            
            if xy_correlation is not None:
                self.assertAlmostEqual(xy_correlation, 1.0, places=3, 
                                     msg=f"å®Œå…¨æ­£ç›¸å…³æ£€æµ‹å¤±è´¥: {xy_correlation}")
                print(f"âœ… å®Œå…¨ç›¸å…³æ£€æµ‹é€šè¿‡: r = {xy_correlation:.6f}")
    
    def test_no_correlation(self):
        """æµ‹è¯•æ— ç›¸å…³çš„æƒ…å†µ"""
        from src.core.analysis_engine import AnalysisEngine, AnalysisConfig
        
        # åˆ›å»ºæ— ç›¸å…³çš„æ•°æ®
        np.random.seed(42)
        x = np.random.normal(0, 1, 1000)
        y = np.random.normal(0, 1, 1000)  # ç‹¬ç«‹éšæœºæ•°
        df = pd.DataFrame({'x': x, 'y': y})
        
        config = AnalysisConfig(
            include_correlation=True,
            correlation_method='pearson'
        )
        engine = AnalysisEngine(config)
        result = engine.analyze(df)
        
        # æ£€æŸ¥ç›¸å…³ç³»æ•°åº”è¯¥æ¥è¿‘0
        corr_matrix = result.correlation_matrix
        if isinstance(corr_matrix, dict):
            xy_correlation = None
            if 'x' in corr_matrix and 'y' in corr_matrix['x']:
                xy_correlation = corr_matrix['x']['y']
            elif 'y' in corr_matrix and 'x' in corr_matrix['y']:
                xy_correlation = corr_matrix['y']['x']
            
            if xy_correlation is not None:
                self.assertLess(abs(xy_correlation), 0.1, 
                               f"æ— ç›¸å…³æ£€æµ‹å¤±è´¥: |r| = {abs(xy_correlation)}")
                print(f"âœ… æ— ç›¸å…³æ£€æµ‹é€šè¿‡: r = {xy_correlation:.6f}")
    
    def test_correlation_methods(self):
        """æµ‹è¯•ä¸åŒç›¸å…³æ€§è®¡ç®—æ–¹æ³•"""
        from src.core.analysis_engine import AnalysisEngine, AnalysisConfig
        
        # åˆ›å»ºå•è°ƒä½†éçº¿æ€§çš„å…³ç³»
        x = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])
        y = x ** 2  # éçº¿æ€§ä½†å•è°ƒå…³ç³»
        df = pd.DataFrame({'x': x, 'y': y})
        
        # æµ‹è¯•Pearsonç›¸å…³ï¼ˆå¯¹éçº¿æ€§ä¸æ•æ„Ÿï¼‰
        config_pearson = AnalysisConfig(
            include_correlation=True,
            correlation_method='pearson'
        )
        engine_pearson = AnalysisEngine(config_pearson)
        result_pearson = engine_pearson.analyze(df)
        
        # æµ‹è¯•Spearmanç›¸å…³ï¼ˆå¯¹å•è°ƒå…³ç³»æ•æ„Ÿï¼‰
        config_spearman = AnalysisConfig(
            include_correlation=True,
            correlation_method='spearman'
        )
        engine_spearman = AnalysisEngine(config_spearman)
        result_spearman = engine_spearman.analyze(df)
        
        print("âœ… ä¸åŒç›¸å…³æ€§æ–¹æ³•æµ‹è¯•å®Œæˆ")


class TestOutlierDetection(unittest.TestCase):
    """æµ‹è¯•å¼‚å¸¸å€¼æ£€æµ‹ç®—æ³•"""
    
    def setUp(self):
        """æµ‹è¯•è®¾ç½®"""
        TestConfig.setup()
    
    def tearDown(self):
        """æµ‹è¯•æ¸…ç†"""
        TestConfig.teardown()
    
    def test_zscore_outlier_detection(self):
        """æµ‹è¯•Z-scoreå¼‚å¸¸å€¼æ£€æµ‹"""
        from src.core.analysis_engine import AnalysisEngine, AnalysisConfig
        
        # åˆ›å»ºåŒ…å«æ˜æ˜¾å¼‚å¸¸å€¼çš„æ•°æ®
        np.random.seed(42)
        normal_data = np.random.normal(0, 1, 100)
        outliers = [10, -10, 15]  # æ˜æ˜¾çš„å¼‚å¸¸å€¼
        data = np.concatenate([normal_data, outliers])
        df = pd.DataFrame({'value': data})
        
        config = AnalysisConfig(
            include_outlier_detection=True,
            outlier_method='zscore',
            outlier_threshold=3.0
        )
        engine = AnalysisEngine(config)
        result = engine.analyze(df)
        
        # éªŒè¯å¼‚å¸¸å€¼æ£€æµ‹ç»“æœ
        outliers_result = result.outliers
        if isinstance(outliers_result, dict):
            outlier_count = outliers_result.get('count', 0)
            self.assertGreater(outlier_count, 0, "åº”è¯¥æ£€æµ‹åˆ°å¼‚å¸¸å€¼")
            
            # å¼‚å¸¸å€¼æ•°é‡åº”è¯¥åˆç†ï¼ˆä¸è¶…è¿‡æ€»æ•°çš„10%ï¼‰
            self.assertLess(outlier_count, len(data) * 0.1, "å¼‚å¸¸å€¼æ£€æµ‹è¿‡äºæ•æ„Ÿ")
            
            print(f"âœ… Z-scoreå¼‚å¸¸å€¼æ£€æµ‹é€šè¿‡: æ£€æµ‹åˆ° {outlier_count} ä¸ªå¼‚å¸¸å€¼")
    
    def test_iqr_outlier_detection(self):
        """æµ‹è¯•IQRå¼‚å¸¸å€¼æ£€æµ‹"""
        from src.core.analysis_engine import AnalysisEngine, AnalysisConfig
        
        # åˆ›å»ºåŒ…å«å¼‚å¸¸å€¼çš„æ•°æ®
        data = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 100]  # 100æ˜¯æ˜æ˜¾å¼‚å¸¸å€¼
        df = pd.DataFrame({'value': data})
        
        config = AnalysisConfig(
            include_outlier_detection=True,
            outlier_method='iqr',
            outlier_threshold=1.5
        )
        engine = AnalysisEngine(config)
        result = engine.analyze(df)
        
        # éªŒè¯å¼‚å¸¸å€¼æ£€æµ‹
        outliers_result = result.outliers
        if isinstance(outliers_result, dict):
            outlier_count = outliers_result.get('count', 0)
            self.assertGreater(outlier_count, 0, "IQRæ–¹æ³•åº”è¯¥æ£€æµ‹åˆ°å¼‚å¸¸å€¼")
            print(f"âœ… IQRå¼‚å¸¸å€¼æ£€æµ‹é€šè¿‡: æ£€æµ‹åˆ° {outlier_count} ä¸ªå¼‚å¸¸å€¼")
    
    def test_outlier_detection_sensitivity(self):
        """æµ‹è¯•å¼‚å¸¸å€¼æ£€æµ‹çš„æ•æ„Ÿæ€§"""
        from src.core.analysis_engine import AnalysisEngine, AnalysisConfig
        
        # åˆ›å»ºæ­£æ€åˆ†å¸ƒæ•°æ®
        np.random.seed(42)
        data = np.random.normal(0, 1, 1000)
        df = pd.DataFrame({'value': data})
        
        # æµ‹è¯•ä¸åŒé˜ˆå€¼çš„æ•æ„Ÿæ€§
        thresholds = [2.0, 3.0, 4.0]
        outlier_counts = []
        
        for threshold in thresholds:
            config = AnalysisConfig(
                include_outlier_detection=True,
                outlier_method='zscore',
                outlier_threshold=threshold
            )
            engine = AnalysisEngine(config)
            result = engine.analyze(df)
            
            outliers_result = result.outliers
            if isinstance(outliers_result, dict):
                count = outliers_result.get('count', 0)
                outlier_counts.append(count)
        
        # éªŒè¯é˜ˆå€¼è¶Šé«˜ï¼Œæ£€æµ‹åˆ°çš„å¼‚å¸¸å€¼è¶Šå°‘
        for i in range(len(outlier_counts) - 1):
            self.assertGreaterEqual(outlier_counts[i], outlier_counts[i + 1],
                                   "é«˜é˜ˆå€¼åº”è¯¥æ£€æµ‹åˆ°æ›´å°‘çš„å¼‚å¸¸å€¼")
        
        print(f"âœ… å¼‚å¸¸å€¼æ£€æµ‹æ•æ„Ÿæ€§æµ‹è¯•é€šè¿‡: é˜ˆå€¼ {thresholds} -> å¼‚å¸¸å€¼ {outlier_counts}")


class TestTimeSeriesAnalysis(unittest.TestCase):
    """æµ‹è¯•æ—¶é—´åºåˆ—åˆ†æç®—æ³•"""
    
    def setUp(self):
        """æµ‹è¯•è®¾ç½®"""
        TestConfig.setup()
    
    def tearDown(self):
        """æµ‹è¯•æ¸…ç†"""
        TestConfig.teardown()
    
    def test_stationarity_test_stationary(self):
        """æµ‹è¯•å¹³ç¨³æ€§æ£€éªŒ - å¹³ç¨³åºåˆ—"""
        from src.core.analysis_engine import AnalysisEngine, AnalysisConfig
        
        # åˆ›å»ºå¹³ç¨³æ—¶é—´åºåˆ—ï¼ˆç™½å™ªå£°ï¼‰
        np.random.seed(42)
        stationary_data = np.random.normal(0, 1, 500)
        
        # æ·»åŠ æ—¶é—´ç´¢å¼•
        dates = pd.date_range('2023-01-01', periods=500, freq='D')
        df = pd.DataFrame({
            'datetime': dates,
            'value': stationary_data
        })
        
        config = AnalysisConfig(
            include_stationarity=True,
            time_column='datetime'
        )
        engine = AnalysisEngine(config)
        result = engine.analyze(df)
        
        # éªŒè¯å¹³ç¨³æ€§æ£€éªŒç»“æœ
        stationarity_result = result.stationarity_test
        if isinstance(stationarity_result, dict):
            is_stationary = stationarity_result.get('is_stationary', False)
            p_value = stationarity_result.get('p_value', 1.0)
            
            self.assertTrue(is_stationary, "å¹³ç¨³åºåˆ—åº”è¯¥è¢«æ£€æµ‹ä¸ºå¹³ç¨³")
            self.assertLess(p_value, 0.05, f"å¹³ç¨³åºåˆ—çš„på€¼åº”è¯¥å°äº0.05: {p_value}")
            print(f"âœ… å¹³ç¨³åºåˆ—æ£€æµ‹é€šè¿‡: p-value = {p_value:.6f}")
    
    def test_stationarity_test_nonstationary(self):
        """æµ‹è¯•å¹³ç¨³æ€§æ£€éªŒ - éå¹³ç¨³åºåˆ—"""
        from src.core.analysis_engine import AnalysisEngine, AnalysisConfig
        
        # åˆ›å»ºéå¹³ç¨³æ—¶é—´åºåˆ—ï¼ˆéšæœºæ¸¸èµ°ï¼‰
        np.random.seed(42)
        random_walk = np.cumsum(np.random.normal(0, 1, 500))
        
        dates = pd.date_range('2023-01-01', periods=500, freq='D')
        df = pd.DataFrame({
            'datetime': dates,
            'value': random_walk
        })
        
        config = AnalysisConfig(
            include_stationarity=True,
            time_column='datetime'
        )
        engine = AnalysisEngine(config)
        result = engine.analyze(df)
        
        # éªŒè¯éå¹³ç¨³æ€§æ£€éªŒç»“æœ
        stationarity_result = result.stationarity_test
        if isinstance(stationarity_result, dict):
            is_stationary = stationarity_result.get('is_stationary', True)
            p_value = stationarity_result.get('p_value', 0.0)
            
            self.assertFalse(is_stationary, "éšæœºæ¸¸èµ°åº”è¯¥è¢«æ£€æµ‹ä¸ºéå¹³ç¨³")
            self.assertGreater(p_value, 0.05, f"éå¹³ç¨³åºåˆ—çš„på€¼åº”è¯¥å¤§äº0.05: {p_value}")
            print(f"âœ… éå¹³ç¨³åºåˆ—æ£€æµ‹é€šè¿‡: p-value = {p_value:.6f}")


class TestDataPreprocessing(unittest.TestCase):
    """æµ‹è¯•æ•°æ®é¢„å¤„ç†ç®—æ³•"""
    
    def setUp(self):
        """æµ‹è¯•è®¾ç½®"""
        TestConfig.setup()
    
    def tearDown(self):
        """æµ‹è¯•æ¸…ç†"""
        TestConfig.teardown()
    
    def test_data_cleaning(self):
        """æµ‹è¯•æ•°æ®æ¸…æ´—ç®—æ³•"""
        from src.data.data_preprocessor import DataPreprocessor
        
        # åˆ›å»ºåŒ…å«ç¼ºå¤±å€¼å’Œé‡å¤å€¼çš„æ•°æ®
        data = pd.DataFrame({
            'A': [1, 2, None, 4, 5, 5],  # åŒ…å«ç¼ºå¤±å€¼å’Œé‡å¤
            'B': [1, 2, 3, 4, 5, 5],
            'C': ['a', 'b', 'c', 'd', 'e', 'e']
        })
        
        preprocessor = DataPreprocessor()
        
        # æµ‹è¯•ç¼ºå¤±å€¼å¤„ç†
        cleaned_data = preprocessor.handle_missing_values(data, method='drop')
        self.assertFalse(cleaned_data.isnull().any().any(), "æ¸…æ´—åä¸åº”æœ‰ç¼ºå¤±å€¼")
        
        # æµ‹è¯•é‡å¤å€¼å¤„ç†
        deduped_data = preprocessor.remove_duplicates(data)
        self.assertEqual(len(deduped_data), 5, "åº”è¯¥ç§»é™¤é‡å¤è¡Œ")
        
        print("âœ… æ•°æ®æ¸…æ´—ç®—æ³•æµ‹è¯•é€šè¿‡")
    
    def test_data_transformation(self):
        """æµ‹è¯•æ•°æ®å˜æ¢ç®—æ³•"""
        from src.data.data_preprocessor import DataPreprocessor
        
        # åˆ›å»ºéœ€è¦å˜æ¢çš„æ•°æ®
        np.random.seed(42)
        data = pd.DataFrame({
            'normal': np.random.normal(100, 15, 1000),
            'skewed': np.random.exponential(2, 1000)
        })
        
        preprocessor = DataPreprocessor()
        
        # æµ‹è¯•æ ‡å‡†åŒ–
        normalized_data = preprocessor.normalize_data(data)
        
        # éªŒè¯æ ‡å‡†åŒ–ç»“æœ
        for col in normalized_data.select_dtypes(include=[np.number]).columns:
            mean = normalized_data[col].mean()
            std = normalized_data[col].std()
            self.assertAlmostEqual(mean, 0, places=1, msg=f"{col}åˆ—å‡å€¼åº”è¯¥æ¥è¿‘0")
            self.assertAlmostEqual(std, 1, places=1, msg=f"{col}åˆ—æ ‡å‡†å·®åº”è¯¥æ¥è¿‘1")
        
        print("âœ… æ•°æ®å˜æ¢ç®—æ³•æµ‹è¯•é€šè¿‡")


class TestNumericalAccuracy(unittest.TestCase):
    """æµ‹è¯•æ•°å€¼è®¡ç®—ç²¾åº¦"""
    
    def test_floating_point_precision(self):
        """æµ‹è¯•æµ®ç‚¹æ•°ç²¾åº¦"""
        from src.core.analysis_engine import AnalysisEngine, AnalysisConfig
        
        # åˆ›å»ºé«˜ç²¾åº¦æ•°å€¼æ•°æ®
        data = pd.DataFrame({
            'precise': [1.123456789, 2.987654321, 3.141592653]
        })
        
        config = AnalysisConfig(include_descriptive=True)
        engine = AnalysisEngine(config)
        result = engine.analyze(data)
        
        # éªŒè¯ç²¾åº¦ä¿æŒ
        stats_dict = result.descriptive_stats.get('precise', {})
        calculated_mean = stats_dict.get('mean', 0)
        expected_mean = data['precise'].mean()
        
        self.assertAlmostEqual(calculated_mean, expected_mean, places=6,
                              msg="é«˜ç²¾åº¦æ•°å€¼è®¡ç®—è¯¯å·®è¿‡å¤§")
        print("âœ… æµ®ç‚¹æ•°ç²¾åº¦æµ‹è¯•é€šè¿‡")
    
    def test_large_numbers(self):
        """æµ‹è¯•å¤§æ•°å€¼å¤„ç†"""
        from src.core.analysis_engine import AnalysisEngine, AnalysisConfig
        
        # åˆ›å»ºå¤§æ•°å€¼æ•°æ®
        large_numbers = [1e9, 1e10, 1e11]
        data = pd.DataFrame({'large': large_numbers})
        
        config = AnalysisConfig(include_descriptive=True)
        engine = AnalysisEngine(config)
        result = engine.analyze(data)
        
        # éªŒè¯å¤§æ•°å€¼è®¡ç®—æ­£ç¡®
        stats_dict = result.descriptive_stats.get('large', {})
        calculated_mean = stats_dict.get('mean', 0)
        expected_mean = np.mean(large_numbers)
        
        relative_error = abs(calculated_mean - expected_mean) / expected_mean
        self.assertLess(relative_error, 1e-10, "å¤§æ•°å€¼è®¡ç®—ç²¾åº¦ä¸è¶³")
        print("âœ… å¤§æ•°å€¼å¤„ç†æµ‹è¯•é€šè¿‡")


def run_algorithm_accuracy_tests():
    """è¿è¡Œç®—æ³•å‡†ç¡®æ€§æµ‹è¯•"""
    print("\nğŸ§® å¼€å§‹æ•°æ®å¤„ç†ç®—æ³•å‡†ç¡®æ€§æµ‹è¯•...")
    
    # åˆ›å»ºæµ‹è¯•å¥—ä»¶
    test_suite = unittest.TestSuite()
    
    # æ·»åŠ æµ‹è¯•ç±»
    test_classes = [
        TestDescriptiveStatistics,
        TestCorrelationAnalysis,
        TestOutlierDetection,
        TestTimeSeriesAnalysis,
        TestDataPreprocessing,
        TestNumericalAccuracy
    ]
    
    for test_class in test_classes:
        tests = unittest.TestLoader().loadTestsFromTestCase(test_class)
        test_suite.addTests(tests)
    
    # è¿è¡Œæµ‹è¯•
    runner = unittest.TextTestRunner(verbosity=2)
    result = runner.run(test_suite)
    
    # ç»Ÿè®¡ç»“æœ
    total_tests = result.testsRun
    failures = len(result.failures)
    errors = len(result.errors)
    passed = total_tests - failures - errors
    
    print(f"\n{'='*60}")
    print(f"ğŸ“Š ç®—æ³•å‡†ç¡®æ€§æµ‹è¯•ç»“æœæ±‡æ€»:")
    print(f"æ€»æµ‹è¯•æ•°: {total_tests}")
    print(f"é€šè¿‡æµ‹è¯•: {passed}")
    print(f"å¤±è´¥æµ‹è¯•: {failures}")
    print(f"é”™è¯¯æµ‹è¯•: {errors}")
    print(f"é€šè¿‡ç‡: {passed/total_tests*100:.1f}%")
    
    if failures > 0:
        print(f"\nâŒ å¤±è´¥çš„æµ‹è¯•:")
        for test, error in result.failures:
            print(f"  - {test}: {error.split('AssertionError:')[-1].strip()}")
    
    if errors > 0:
        print(f"\nğŸ’¥ é”™è¯¯çš„æµ‹è¯•:")
        for test, error in result.errors:
            print(f"  - {test}: {error.split('Exception:')[-1].strip()}")
    
    if passed == total_tests:
        print("ğŸ‰ æ‰€æœ‰ç®—æ³•å‡†ç¡®æ€§æµ‹è¯•é€šè¿‡ï¼")
        return True
    else:
        print("âš ï¸  éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œéœ€è¦æ£€æŸ¥ç®—æ³•å®ç°")
        return False


if __name__ == "__main__":
    success = run_algorithm_accuracy_tests()
    sys.exit(0 if success else 1)